# Character Level Language Models

Recurrent Neural Networks, or RNNs are one of the most influential architecture types. It inspired several different architecture types (e.g., LSTM, GRUs, stacked RNNs)
that later lead to the key ideas in the Transformer architecture. Therefore, this project implements several different RNN-based architecture types.  

As a use-case, the task of a character-level language model is selected. It enables efficient demonstration of the work of the RNNs. 

This project implements the following papers: 

1. LSTM **Long short-term memory.** Hochreiter, S., & Schmidhuber, J. (1997). Neural computation, 9(8), 1735–1780.
2. GRU **On the properties of neural machine translation: encoder-decoder approaches.** Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014).  arXiv preprint arXiv:1409.1259.
3. RNN **Learning internal representations by error propagation.** Rumelhart, David E; Hinton, Geoffrey E, and Williams, Ronald J (Sept. 1985). Tech. rep. ICS 8504. San Diego, California: Institute for Cognitive Science, University of California.
4. Stacked RNN **Learning internal representations by error propagation.** Rumelhart, David E; Hinton, Geoffrey E, and Williams, Ronald J (Sept. 1985). Tech. rep. ICS 8504. San Diego, California: Institute for Cognitive Science, University of California.
5. Bidirectional RNN **Bidirectional recurrent neural networks.** Schuster, M., & Paliwal, K. K. (1997). IEEE Transactions on Signal Processing, 45(11), 2673–2681.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "\n",
    "from model import UNet\n",
    "from data import LandscapeData\n",
    "from trainer import DiffussionTrainer\n",
    "from diffussion import Diffussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in = 3\n",
    "c_out = 3\n",
    "time_dim = 128\n",
    "lr = 0.001\n",
    "max_epochs = 1\n",
    "batch_size = 3\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LandscapeData(dataset_path=\"archive\", image_size=image_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = Diffussion()\n",
    "model = UNet(c_in=c_in, c_out=c_out, lr=lr, time_dim=time_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DiffussionTrainer(max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.inc = DoubleConv(c_in, 64)\n",
    "# self.down1 = Down(64, 128) # (input output)\n",
    "# self.sa1 = SelfAttention(128, 32) # (chanel dimension, image resolution)\n",
    "# self.down2 = Down(128, 256)\n",
    "# self.sa2 = SelfAttention(256, 16)\n",
    "# self.down3 = Down(256, 256)\n",
    "# self.sa3 = SelfAttention(256, 8)\n",
    "\n",
    "\n",
    "# self.bot1 = DoubleConv(256, 512)\n",
    "# self.bot2 = DoubleConv(512, 512)\n",
    "# self.bot3 = DoubleConv(512, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently running 1\n",
      "torch.Size([3, 3, 64, 64]) torch.Size([3, 128])\n",
      "input convolution  torch.Size([3, 64, 64, 64])\n",
      "first down convolution  torch.Size([3, 128, 32, 32])\n",
      "Input attention  torch.Size([3, 128, 32, 32])\n",
      "Input attention after view  torch.Size([3, 1024, 128])\n",
      "Input attention after view  torch.Size([3, 128, 32, 32])\n",
      "first self attention  torch.Size([3, 128, 32, 32])\n",
      "second down convolution torch.Size([3, 256, 16, 16])\n",
      "Input attention  torch.Size([3, 256, 16, 16])\n",
      "Input attention after view  torch.Size([3, 256, 256])\n",
      "Input attention after view  torch.Size([3, 256, 16, 16])\n",
      "second self attention  torch.Size([3, 256, 16, 16])\n",
      "third down convoluiton  torch.Size([3, 256, 8, 8])\n",
      "Input attention  torch.Size([3, 256, 8, 8])\n",
      "Input attention after view  torch.Size([3, 64, 256])\n",
      "Input attention after view  torch.Size([3, 256, 8, 8])\n",
      "third self-attention  torch.Size([3, 256, 8, 8])\n",
      "first botlenach input  torch.Size([3, 256, 8, 8])\n",
      "second botlenach input  torch.Size([3, 512, 8, 8])\n",
      "third botlenach input  torch.Size([3, 512, 8, 8])\n",
      "third botlenach output  torch.Size([3, 256, 8, 8])\n",
      "torch.Size([3, 256, 8, 8])\n",
      "torch.Size([3, 256, 16, 16])\n",
      "torch.Size([3, 512, 16, 16])\n",
      "torch.Size([3, 128, 16, 16])\n",
      "first up  torch.Size([3, 256, 8, 8]) torch.Size([3, 256, 16, 16]) torch.Size([3, 128])\n",
      "Input attention  torch.Size([3, 128, 16, 16])\n",
      "Input attention after view  torch.Size([3, 256, 128])\n",
      "Input attention after view  torch.Size([3, 128, 16, 16])\n",
      "self-attention 4  torch.Size([3, 128, 16, 16])\n",
      "torch.Size([3, 128, 16, 16])\n",
      "torch.Size([3, 128, 32, 32])\n",
      "torch.Size([3, 256, 32, 32])\n",
      "torch.Size([3, 64, 32, 32])\n",
      "second up  torch.Size([3, 64, 32, 32]) torch.Size([3, 128, 32, 32]) torch.Size([3, 128])\n",
      "Input attention  torch.Size([3, 64, 32, 32])\n",
      "Input attention after view  torch.Size([3, 1024, 64])\n",
      "Input attention after view  torch.Size([3, 64, 32, 32])\n",
      "torch.Size([3, 64, 32, 32])\n",
      "torch.Size([3, 64, 64, 64])\n",
      "torch.Size([3, 128, 64, 64])\n",
      "torch.Size([3, 64, 64, 64])\n",
      "Input attention  torch.Size([3, 64, 64, 64])\n",
      "Input attention after view  torch.Size([3, 4096, 64])\n",
      "Input attention after view  torch.Size([3, 64, 64, 64])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/matilda/PycharmProjects/machine_learning_methods/Diffussion_models/main.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matilda/PycharmProjects/machine_learning_methods/Diffussion_models/main.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(data\u001b[39m=\u001b[39;49mdata, model\u001b[39m=\u001b[39;49mmodel, diffusion\u001b[39m=\u001b[39;49mdiffusion)\n",
      "File \u001b[0;32m~/PycharmProjects/machine_learning_methods/Diffussion_models/trainer.py:117\u001b[0m, in \u001b[0;36mDiffussionTrainer.fit\u001b[0;34m(self, data, model, diffusion)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[1;32m    116\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcurrently running \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_epoch()\n",
      "File \u001b[0;32m~/PycharmProjects/machine_learning_methods/Diffussion_models/trainer.py:132\u001b[0m, in \u001b[0;36mDiffussionTrainer.fit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiffusion\u001b[39m.\u001b[39msample_timestamps(images\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiffusion\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    131\u001b[0m x_t, noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiffusion\u001b[39m.\u001b[39mnoise_image(images, t)\n\u001b[0;32m--> 132\u001b[0m predicted_noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x_t, t)\n\u001b[1;32m    133\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mloss(noise, predicted_noise)\n\u001b[1;32m    134\u001b[0m l\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/PycharmProjects/DeepLearning/miniconda39/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/machine_learning_methods/Diffussion_models/model.py:343\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    338\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup3(x, x1, t)\n\u001b[1;32m    340\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa6(x)\n\u001b[0;32m--> 343\u001b[0m exit(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    344\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutc(x)\n\u001b[1;32m    346\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(data=data, model=model, diffusion=diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
